<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ben Ashford</title>
    <description>Mildly interesting technology stories.  Clojure, Rust and anything else.
</description>
    <link>http://benashford.github.io/</link>
    <atom:link href="http://benashford.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 11 Sep 2016 20:09:39 +0100</pubDate>
    <lastBuildDate>Sun, 11 Sep 2016 20:09:39 +0100</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Building an Event-based Redis Client in Java</title>
        <description>&lt;h2 id=&quot;the-story-so-far&quot;&gt;The story so far&lt;/h2&gt;

&lt;p&gt;(You may want to skip this part, if you have already read my previous blog post on the subject: &lt;a href=&quot;/blog/2015/06/02/java-in-a-polygot-jvm-world&quot;&gt;Java in a polyglot JVM world&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;A few months ago, I had an idea for a project that required a certain amount of data-processing.  Nothing very sophisticated, but potentially very time consuming.  As with a lot of projects, there were a wealth of open-source libraries that would get me 95% of the way; these could be integrated into a pipeline based on Clojure’s &lt;code&gt;core.async&lt;/code&gt;.  However one thing was missing: state, each process (there may have been more than one) would need to know what work remained; if this state were lost, everything would have to begin from scratch.&lt;/p&gt;

&lt;p&gt;As things happened, I quickly realised the scope was much smaller than I had reckoned, so I quickly reached my goal with a single Ruby script and a SQLite database.  But the seed had been sewn, I wanted a way of talking to Redis is a non-blocking way, so it could be integrated with Clojure’s &lt;code&gt;core.async&lt;/code&gt;.  That quickly became the &lt;a href=&quot;https://github.com/benashford/redis-async&quot;&gt;&lt;code&gt;redis-async&lt;/code&gt; project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, the initial performance of this project was poor.  Poor enough, compared to pre-existing non-async Redis clients that it nullified the existence of the project, no-one (not even me) would use it in that state.  So I began to investigate the poor performance, and fix things.  This gave birth to the &lt;a href=&quot;https://github.com/benashford/jresp&quot;&gt;JRESP project&lt;/a&gt;.  The process and rationale of this changes I described in the earlier blog post: &lt;a href=&quot;/blog/2015/06/02/java-in-a-polygot-jvm-world&quot;&gt;Java in a polyglot JVM world&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This led to much improved performance for &lt;code&gt;redis-async&lt;/code&gt;, with most of the high-level code in Clojure but the low-level code (serialisation, sockets, etc.) in Java.&lt;/p&gt;

&lt;p&gt;But there was still more to do…&lt;/p&gt;

&lt;p&gt;Performance was still an order of magnitude poorer than the competition.  I had many theories for this, mostly revolving around two themes: 1) the more complex nature of parsing results in an event-based system, you can’t just read bytes from an &lt;code&gt;InputStream&lt;/code&gt;; and, 2) the complex handling of multiplexing many Redis commands onto a single connection with implicit pipelining.&lt;/p&gt;

&lt;p&gt;This gave me two avenues to explore to make things better.&lt;/p&gt;

&lt;h2 id=&quot;why-pipelining&quot;&gt;Why pipelining?&lt;/h2&gt;

&lt;p&gt;To make the most efficient use of the connection.  Each Redis command is (typically, there are some exceptions) small; rather than waiting for a response for each command, it is more efficient to send multiple commands, then wait for all of the corresponding responses.  The official Redis docs have a &lt;a href=&quot;http://redis.io/topics/pipelining&quot;&gt;good explanation of this, and benchmarks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most traditional clients make pipelining the responsibility of the application developer.  Typically by declaring a block that contains multiple Redis commands, the block then returns a single collection containing all of the results&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The advantage of an asynchronous and/or event-based approach is that pipelining can be achieved done automatically&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.  If an application is issuing ten commands in quick succession, since it doesn’t wait for responses immediately these can all be combined into a single request/response cycle.  This should make the application code cleaner, and also, potentially, discover additional efficiencies if there are multiple threads issuing commands at the same time, they could all be combined into one single request/response cycle.&lt;/p&gt;

&lt;p&gt;The downside is that none of this can be guaranteed, since the code cannot know when these breaks are.  Instead it relies on the principal that issuing commands will be faster than writing to a socket, so each socket write will be fully utilised in the sense that all commands issued in the interim will be included.  But there will undoubtedly be some edge-cases where the worst-case occurs.&lt;/p&gt;

&lt;p&gt;As with most different approaches to common problems, there are tradeoffs, and there is an upside to the non-determinism.  In the case of very-large numbers of commands in a single pipeline (hundreds of thousands), there is a latency problem in existing synchronous clients - the first command won’t be actually sent until all the commands have been issued, and the first response won’t be parsed until all commands have been sent - but with implicit pipelining the stream will be naturally batched into smaller chunks, something that would have to be done manually using other approaches.&lt;/p&gt;

&lt;p&gt;The goal, therefore, is that using JRESP and &lt;code&gt;redis-async&lt;/code&gt; in a naive way will be more efficient than using existing synchronous clients in a naive way, even if you use pipelining.&lt;/p&gt;

&lt;h2 id=&quot;implicit-pipelining&quot;&gt;Implicit pipelining&lt;/h2&gt;

&lt;p&gt;The way &lt;code&gt;redis-async&lt;/code&gt; implemented implicit pipelining was at the &lt;code&gt;core.async&lt;/code&gt; layer.  Essentially whenever an outgoing command went through the outgoing (buffered) channel, before sending it to Redis (via JRESP) any other commands on the channel were also taken (but it wouldn’t wait for any more), these would be read into a vector which would be passed to JRESP.  This still had the latency problem if there was a large number of commands.&lt;/p&gt;

&lt;p&gt;The most recent change has been to move this responsibility out of the Clojure layer and into the Java layer.  This has resulted in two key benefits: 1) a great simplification of the Clojure code, there’s no need for an outgoing channel anymore, each Redis command is a thin wrapper calling the underling JRESP &lt;code&gt;write&lt;/code&gt; method; and, 2) it means the implicit pipelining can be done in sync with the low-level socket signalling using Java’s NIO package.&lt;/p&gt;

&lt;p&gt;Now, when a command is issued from &lt;code&gt;redis-async&lt;/code&gt; this corresponds with a single method call to JRESP.  The command will be serialised into a &lt;code&gt;java.nio.ByteBuffer&lt;/code&gt; and added to a queue.  The queue compacts multiple byte buffers up to a maximum size - essentially the size of one TCP packet.  Each connection pool (a new addition to JRESP) has a thread dedicated to all the sockets it controls, this uses a &lt;code&gt;java.nio.channels.Selector&lt;/code&gt; and the state of each queue to be notified when each socket is ready for reading or writing.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This new approach has resulted in a significant gain in throughput compared to previously.  I’m planning on doing a comprehensive set of performance tests, but so far been using the same micro-benchmarks I mentioned in the previous post.  On those measures &lt;code&gt;redis-async&lt;/code&gt; performance is between 10% faster and 100% slower than Carmine; although, interestingly, JRESP alone seems to be 25% faster than Jedis.  These numbers feel counter intuitive somehow, as either Carmine is faster than Jedis, which other benchmarks show not to be the case, or this heavily implies the difference in performance is due to the Clojure layer of &lt;code&gt;redis-async&lt;/code&gt; above JRESP&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;… but I need to investigate more to both prove these interim results and to provide enough test cases to improve things further, the micro-benchmarks are too unrealistic to draw conclusions.&lt;/p&gt;

&lt;p&gt;But, what is significant is the progress of improving performance as I’ve gone though these refinements.  From one hundred times slower to more-or-less&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; the same speed.&lt;/p&gt;

&lt;p&gt;The final question, what’s to stop JRESP being a full-blown Redis client in its own right?  Not a lot actually, it now has all of the requisite parts, but it’s not a particular priority given the wealth of alternative Java clients.  But all that is needed is a facade class containing methods for each Redis command, in &lt;code&gt;redis-async&lt;/code&gt; this is automatically generated from the &lt;a href=&quot;https://github.com/antirez/redis-doc/blob/master/commands.json&quot;&gt;list of commands provided by redis-doc&lt;/a&gt;; obviously the meta-programming nature of Clojure makes this easy, I’ll need to find use an external tool to do the same with JRESP.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;The previous link to the Redis docs also contain code showing this in effect. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;code&gt;redis-async&lt;/code&gt;’s README shows proof and examples of this: &lt;a href=&quot;https://github.com/benashford/redis-async#implicit-pipelining&quot;&gt;https://github.com/benashford/redis-async#implicit-pipelining&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;My immediate guess is that &lt;code&gt;redis-async&lt;/code&gt; is slower at issuing the commands; but JRESP will be sending data to Redis at the same speed.  The end result being less efficient throughput. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Within one order-of-magnitude. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 13 Sep 2015 13:35:21 +0100</pubDate>
        <link>http://benashford.github.io/blog/2015/09/13/building-an-event-based-redis-client-in-java</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2015/09/13/building-an-event-based-redis-client-in-java</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Java in a polyglot JVM world</title>
        <description>&lt;p&gt;The trend over the last few years has been clear, the &lt;a href=&quot;http://redmonk.com/dberkholz/2014/05/02/github-language-trends-and-the-fragmenting-landscape/&quot;&gt;number of viable platforms from which to build software systems has been growing&lt;/a&gt;.  Not that many years ago, things would be either: Java, C++, Visual Basic or one of the many long-forgotten but once popular languages like Delphi.  These days, things are either: JavaScript, Java (also: Clojure, Scala, Kotlin, Groovy), .NET (C#, F#), Ruby (including JRuby), Python (2 and 3, and all the interesting Python community sub-projects like PyPy and rpython), Rust, Go, C, C++, Haskell, OCaml, Erlang (and Elixir), Nim, Crystal, it goes on and on…  Not every language is suitable to every task, but there’s a surprisingly wide central area of suitability that nearly all of the above overlap.&lt;/p&gt;

&lt;p&gt;Despite many online programming forums regressing to a continual flame-war on the subject of “which language will win”&lt;sup id=&quot;fnref:0&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, it’s quite clear this fragmentation&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; is here to stay, at least for the forseeable future.  Coupled with this is the habit of programmer communities forming language-tribes, which is a great shame as real-world code suffers as a result of tribal dogma&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:2a&quot;&gt;&lt;a href=&quot;#fn:2a&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;I would be suspicious of any programmer who claimed to be an expert in all these programming languages, unless their name were Fabrice Bellard to TJ Holowaychuk perhaps.  But I firmly believe in the principal of understanding multiple ways of doing things, there is no one-true-answer to everything (although there are nearly-one-true-answers to some things).&lt;/p&gt;

&lt;p&gt;So, in todays diverse programming world, in which situations would you use an old, unfashionable, language like Java?  Especially if you’re already using a modern, fashionable, language like Clojure.  They both target the JVM, so surely you’d use the newer one, unless there was an explicit need for performance.  Actually, I would say no, there are a lot of problem-domains for which Java is very well suited.&lt;/p&gt;

&lt;h3 id=&quot;an-example&quot;&gt;An example&lt;/h3&gt;

&lt;p&gt;First, why Clojure?  There has been a metaphorical avalanche of praise for Clojure the past few years, so I won’t repeat the full list, not least because the full list includes things I don’t really agree with; but there are many clear-cut advantages that Clojure has.  By all means this list isn’t unique to Clojure, pretty much everything in it also exists in at least one other language, and the benefits don’t apply equally to all application domains; but the fact that they exist together, in a language that targets ubiquitous platforms (JVM, JavaScript, etc.) makes it a very good general-purpose programming language.  For the record, I believe the key benefits of Clojure are: a) functional programming, not just a “functional optional” approach like Scala&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;; b) immutable data-structures and language support for STM, etc.; and c) macros&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, and how that enables &lt;code&gt;core.async&lt;/code&gt; and other such libraries.&lt;/p&gt;

&lt;p&gt;A few months ago I had an idea for a side-project that required a bit of data-mining.  My initial idea for the architecture was to use &lt;code&gt;core.async&lt;/code&gt; as there were excellent asynchronous libraries available for both ends of the pipeline; however in the middle, to keep track of state and various pieces of transient data, I planned to use Redis.  As it happened, I quickly realised the size of the data was small, so I implemented the whole thing in-memory in a single Ruby script instead, but that’s another story; by this time the problem of how to talk to Redis in an application heavily based around &lt;code&gt;core.async&lt;/code&gt; was firmly in my mind, so I deced to do something about it.&lt;/p&gt;

&lt;p&gt;I’m aware of half-a-dozen Redis clients for Clojure, and have used a couple of those.  The current market leader is the excellent &lt;a href=&quot;https://github.com/ptaoussanis/carmine&quot;&gt;Carmine&lt;/a&gt;&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, it’s fast, it’s complete, what’s not to like?  However it does use blocking IO for most operations.  Since beginning on &lt;a href=&quot;https://github.com/benashford/redis-async&quot;&gt;my client&lt;/a&gt; I have become aware of other experimental forays into this area, but I wasn’t aware of them at the time, and none of them look like the finished article.&lt;/p&gt;

&lt;p&gt;(Incidentally, Carmine is a good project to read if you want to know how to write good idiomatic Clojure.)&lt;/p&gt;

&lt;h3 id=&quot;redis-async&quot;&gt;redis-async&lt;/h3&gt;

&lt;p&gt;My project &lt;a href=&quot;https://github.com/benashford/redis-async&quot;&gt;&lt;code&gt;redis-async&lt;/code&gt;&lt;/a&gt; quickly gained shape in my mind.  It had the following design goals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To be a complete Redis client.&lt;/li&gt;
  &lt;li&gt;To be asynchronous, specifically non-blocking to any consumers.&lt;/li&gt;
  &lt;li&gt;To use &lt;code&gt;core.async&lt;/code&gt; features (e.g. channels) rather than other mechanisms (e.g. callbacks or promises).  This was mainly as I envisaged the need to integrate with other &lt;code&gt;core.async&lt;/code&gt;-based code.  Internally there may be callbacks, but the external interface would be channels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The complications were many and varied too.  Not least the need to square a few circles regarding idiomacy; for example: a Redis string is what the Java/Clojure world would call a byte array - but yet most uses would require a Clojure/Java string; however there are some Redis commands where a consumer would require a byte array (&lt;code&gt;DUMP&lt;/code&gt; and &lt;code&gt;RESTORE&lt;/code&gt; plus others), so automatically converting would be incorrect.&lt;/p&gt;

&lt;p&gt;The initial decision was how to go about this.  There were three strands that went through my mind:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Forget about the non-blocking IO entirely, Redis is fast, it’s unlikely to cause deadlock, it’ll just lead to an inefficient use of the threadpool that backs Go blocks.  (I quickly dismissed this as being a dangerous path, there’s no reason why talking to Redis couldn’t be implemented asynchronously.)&lt;/li&gt;
  &lt;li&gt;To use &lt;code&gt;core.async&lt;/code&gt; channels to communicate with one-or-more threads (i.e. full threads, not go blocks) that call Carmine functions.  (Tempting, but again the Redis protocol isn’t that complicated that the lack of an off-the-shelf library is a show-stopper.)&lt;/li&gt;
  &lt;li&gt;Implement the Redis protocol myself, and use a low-level non-blocking library for the networking.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I chose Option 3.&lt;/p&gt;

&lt;p&gt;The single major difference between synchronous and asynchronous communication, is how you go about reading the responses.  Synchronously you would read from an &lt;code&gt;InputStream&lt;/code&gt; until you have the data you are expecting (Redis is deterministic in this regard); if the data is not yet available, the thread will block until it is.  Asynchronously, you will receieve a sequence of events, each of which contain an arbitrary amount of data; each event could contain multiple messages, but also an event may contain only part of a message; client code needs to combine multiple events to reconsititute a single message.&lt;/p&gt;

&lt;h4 id=&quot;the-redis-protocol&quot;&gt;The Redis protocol&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;http://redis.io/topics/protocol&quot;&gt;Redis protocol&lt;/a&gt;, known as RESP, is designed to be simple to implement, that must be true because that’s what it says at the top of its own documentation.  It’s made-up of a few simple types covering strings (byte-arrays), integers, arrays etc.  It has a few fun peculiarities, like the fact that numbers are transmitted as strings, but this was done to make it human readable.&lt;/p&gt;

&lt;p&gt;Communication both to and from a Redis server is a stream of RESP objects.  A Redis command is just a RESP array where the elements are the command plus any options, each encoded as RESP.  E.g. &lt;code&gt;SET KEY-NAME VALUE&lt;/code&gt; is sent over the wire as &lt;code&gt;*3\r\n$3\r\nSET\r\n$8\r\nKEY-NAME\r\n$5\r\nVALUE\r\n&lt;/code&gt;&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.  Responses are similarly encoded, but the exact response varies according to the command; you can be sure that one command results in one response…&lt;/p&gt;

&lt;p&gt;…except for those that don’t.  There are a number of commands with different semantics.  Some like the “blocking” commands (e.g. &lt;a href=&quot;http://redis.io/commands/BLPOP&quot;&gt;&lt;code&gt;BLPOP&lt;/code&gt;&lt;/a&gt;) still return one command, but the client will have to wait for it if it’s not already there; others like the various &lt;a href=&quot;http://redis.io/topics/pubsub&quot;&gt;Pub/Sub&lt;/a&gt; commands will subscribe you to a channel and there will be an indefinite number of responses.&lt;/p&gt;

&lt;h4 id=&quot;the-first-iteration&quot;&gt;The first iteration&lt;/h4&gt;

&lt;p&gt;In any application space there are a number of libraries that are so widely adopted they’re the de-facto choice.  For asynchronous networking in Clojure, that library is &lt;a href=&quot;https://github.com/ztellman/aleph&quot;&gt;Aleph&lt;/a&gt;, which in-turn is based around the venerable Java library &lt;a href=&quot;http://netty.io&quot;&gt;Netty&lt;/a&gt;.  Zach Tellman also owns a number of other good libraries which I would use in this first iteration, in particular &lt;a href=&quot;https://github.com/ztellman/gloss&quot;&gt;Gloss&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gloss was used to implement the Redis protocol, and Aleph used to transmit data back-and-forth.  Gloss is very easy to integrate with Aleph and it handles the problem of messages being split over multiple callbacks, my code just read fully formed RESP messages.&lt;/p&gt;

&lt;p&gt;Once this was up-and-running it was time for a quick benchmark against Carmine.  Now, before I tell you the numbers, let me get my excuses in first: I was expecting it to be slower, for many reasons, but mainly two:&lt;/p&gt;

&lt;p&gt;One, Carmine is very fast&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;, and the &lt;a href=&quot;https://github.com/ptaoussanis/carmine/blob/master/src/taoensso/carmine/protocol.clj&quot;&gt;code it uses for reading and parsing RESP&lt;/a&gt; is very clean and straight forward.  It essentially comes down to &lt;a href=&quot;https://github.com/ptaoussanis/carmine/blob/master/src/taoensso/carmine/protocol.clj#L123-L189&quot;&gt;one function&lt;/a&gt; that reads the data from a Java &lt;code&gt;InputStream&lt;/code&gt; into precise chunks.  Asynchronous code would have to take a different approach, messages could be split across packets; even though this was handled for me by Gloss, it was still a runtime cost.&lt;/p&gt;

&lt;p&gt;Two, Carmine is very direct.  &lt;code&gt;redis-async&lt;/code&gt; on the other hand would, by design, receive this incoming piece of data, route it to the waiting channel; somewhere else, in another thread, the arrival of this message would trigger the resumption of a go-block waiting on that channel; finally that code would do whatever it needed to do with the result.  There was bound to be an overhead to all this.&lt;/p&gt;

&lt;p&gt;So the results?  First, the experiment: it was very basic, I would send 1,000 &lt;code&gt;PING&lt;/code&gt; commands and wait for 1,000 &lt;code&gt;PONG&lt;/code&gt;s to be returned.&lt;/p&gt;

&lt;p&gt;Carmine achieved this in 2.66 milliseconds.  &lt;code&gt;redis-async&lt;/code&gt;, well, let’s just say it had a 2 in it… it was in the hundreds of milliseconds.&lt;/p&gt;

&lt;p&gt;Superficially they were doing the same thing, both were pipelining their commands.  Carmine was returning a single result as a vector, however &lt;code&gt;redis-async&lt;/code&gt; was distributing 1000 &lt;code&gt;PONG&lt;/code&gt;s to their respective channels, which were then being waited on to be sure all the results were in.  It was obvious that would take some time, but not 99% of a couple of hundred milliseconds worth.  Surely?&lt;/p&gt;

&lt;p&gt;It’s also true-to-say that the chosen experiment was the one most likely to flatter Carmine given what was known about the implementation, and a more typical test would have produced a narrower result.  But the gap was so massive I decided not to equivocate, the baseline would have to be improved either way.&lt;/p&gt;

&lt;h4 id=&quot;the-second-iteration&quot;&gt;The second iteration&lt;/h4&gt;

&lt;p&gt;A series of isolated tests on the individual layers showed that it was the de-serialization steps, going from binary to records that described the RESP types, was where the most time was being spent.  And even more specifically, it was RESP’s “simple strings” that were the problem.  In my isolated test, replacing them with bulk strings made everything many times faster.  But, since simple strings were in the protocol, this would have to be addressed.&lt;/p&gt;

&lt;p&gt;I decided to drop Gloss, and implement the protocol myself.  The reason was that I was rapidly accumulating extra cases to handle RESP, and the weight of these meant that Gloss was no-longer the simplest solution; the logic was lost amongst the workarounds.&lt;/p&gt;

&lt;p&gt;The second iteration still used Aleph, but took the raw byte data and did all the parsing (and handling split messages) itself.&lt;/p&gt;

&lt;p&gt;The results for the second iteration were much improved, but not stellar.  It was in the low hundreds of milliseconds initially, which after a few tweaks, I got down to the ~90ms mark.&lt;/p&gt;

&lt;h3 id=&quot;where-java-fits-in-the-modern-polyglot-jvm-based-world&quot;&gt;Where Java fits in the modern polyglot JVM-based world&lt;/h3&gt;

&lt;p&gt;Looking at &lt;a href=&quot;https://github.com/benashford/redis-async/blob/c0e34395bd20c2b0a84c47ddca4a7fdb6eb2da04/src/redis_async/protocol.clj&quot;&gt;the code I had written&lt;/a&gt; there was much to dislike.  First, there was a lot of it for such a simple protocol; it was very low-level dealing with individual bytes and byte-buffers; it wasn’t functionally pure, the buffers were mutable; there was a lot of other state, but all of it was passed around as a parameter.&lt;/p&gt;

&lt;p&gt;The conclusion to this was obvious: why don’t I just write it in Java?  The Clojure version could be improved, just as the Gloss version in the previous iteration could have been; however, the direction I wanted to take (in both cases) was to move further away from the strength of those technologies.  The game was stateful byte-by-byte manipulation, I might as well choose a starting point that makes that game easier to play.&lt;/p&gt;

&lt;h4 id=&quot;the-third-iteration&quot;&gt;The third iteration&lt;/h4&gt;

&lt;p&gt;I decided to split &lt;code&gt;redis-async&lt;/code&gt; in two.  Everything that was in &lt;code&gt;protocol.clj&lt;/code&gt; or lower would be re-written in Java; everything else would stay in Clojure.  This would mean the &lt;a href=&quot;https://github.com/benashford/jresp&quot;&gt;Java library, or JRESP - Java RESP&lt;/a&gt; - would provide objects representing each RESP type (Array, Bulk String, Simple String, etc.), and also the means to read and write them to Redis instance.&lt;/p&gt;

&lt;p&gt;Aleph was removed as a dependency, but Netty is still used by the Java code.  What was the result?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There’s less code.  You would expect a reduction of code going from Java to Clojure, not the other way around.  The code is spread out over a much wider area however, on account of Java’s one-public-class-per-file rule.&lt;/li&gt;
  &lt;li&gt;The code is conceptually simpler.  The structure is obvious, and the areas were significant events occur (e.g. parsing data) are easily identified.&lt;/li&gt;
  &lt;li&gt;The state is held by the language making the logic easier to follow.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Clojure parts now simply call the &lt;code&gt;write&lt;/code&gt; method on the &lt;code&gt;Connection&lt;/code&gt; object to send a set of commands, the Clojure code also implements the callback from the &lt;code&gt;Connection&lt;/code&gt; object, writing the result to the &lt;code&gt;core.async&lt;/code&gt; channel that is used to allocate results back to specific channels.&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;So what about the performance?  Well, there’s good and bad news.  The bad news is that the full-client is still slower than Carmine, but the time for 1000 &lt;code&gt;PING&lt;/code&gt;/&lt;code&gt;PONG&lt;/code&gt;s is under 50ms.  The good news is that, by timing just the Java layer, the speed is significantly improved.  Measured at that layer 1000 pings is 2.9ms.&lt;/p&gt;

&lt;p&gt;Both the Java version, and it’s Clojure predecessor, is unoptimised; so I have no doubt both could be made faster still.  But comparing them both as initial versions is still interesting.  While the revelation that Java is faster than Clojure will come as a shock to nobody, it can still be worth measuring as the gains are non-linear; and still the JRESP and Carmine code are both very similar (although JRESP is still doing more work to asynchronously read the result, albeit a small amount of extra work).&lt;/p&gt;

&lt;p&gt;As for the remaining 47.5ms, which is spent entirely at the Clojure layer, I haven’t investigated yet.  It could be that this is the overhead of &lt;code&gt;core.async&lt;/code&gt;.  It could also be that the nature of the 1000 ping test is untypical and I should do more typical tests before coming to conclusions.  Only then would I be able to put a price on this approach, then decide if such a cost actually works for the type of applications likely to use it.&lt;/p&gt;

&lt;p&gt;The version of &lt;code&gt;redis-async&lt;/code&gt; with JRESP is still in development.  I’m intending on doing more performance testing and testing of edge-cases before releasing it.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The conclusion, like all good conclusions, is retrospectively obvious: low-level logic is a good candidate for low-level languages.  Perhaps not so obvious, especially with the modern zeitgeist of developer productivity being linked to expressiveness, is that low-level programming languages are the best medium for expressing low-level logic.&lt;/p&gt;

&lt;p&gt;Java’s role is essentially to be the C of the JVM.  For writing code that is low-level, needs to be performant and/or needs to be shared across multiple JVM languages; it would be relatively easy to create a Scala Redis client based on JRESP, and even a full Java client for that matter.&lt;/p&gt;

&lt;h5 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h5&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:0&quot;&gt;
      &lt;p&gt;Often this is a subtext of the actual topic at hand. &lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;I prefer the term “diversity”. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Once overheard: “we can’t do that!  If we use a SQL database, we’re only one step away from Java!”  All the while our competitors had a Java application based around a SQL database nicely satisfying all our potential customers without complaint… &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2a&quot;&gt;
      &lt;p&gt;Often such tribalism is rational, given those lines are often used in hiring practices. &lt;a href=&quot;#fnref:2a&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Although that might be beneficial for other reasons, at least the intent is clear. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Dangerous, but revolutionary in the right hands. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;One of those impossible-to-Google project names. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Yes, it’s longer than just sending the string, this is normal; but it has removed ambiguity. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;For bulk string operations, those beginning with &lt;code&gt;$&lt;/code&gt;, you are told the number of bytes to read but yet there’s still a &lt;code&gt;\r\n&lt;/code&gt; at the end.  The final &lt;code&gt;\r\n&lt;/code&gt; is a bit of a nuisance, as you need to add two to the number of bytes to read, then remove it again. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;As I said before, it’s a very good project to read if you want to learn idiomatic (yet high performance) Clojure code.  There’s no fat in the entire project. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;&lt;em&gt;Warning&lt;/em&gt;: The Java code does not yet enforce any thread-safety, this is because the way it is currently used is guarenteed to come from only one thread at a time.  This will change in the near future however, as this is an implementation detail of the Clojure implementation, and is likely not to be true for any other user of that library. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 02 Jun 2015 00:00:00 +0100</pubDate>
        <link>http://benashford.github.io/blog/2015/06/02/java-in-a-polygot-jvm-world</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2015/06/02/java-in-a-polygot-jvm-world</guid>
        
        <category>clojure</category>
        
        <category>java</category>
        
        <category>jvm</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Rust traits for developer friendly libraries</title>
        <description>&lt;p&gt;For the last six months or so, I’ve been looking more-and-more into &lt;a href=&quot;/blog/2014/12/21/rust/&quot;&gt;Rust&lt;/a&gt;, and the more I look into it the more I like.&lt;/p&gt;

&lt;p&gt;My latest Rust project has been to implement a &lt;a href=&quot;https://github.com/benashford/rs-es&quot;&gt;client to the ElasticSearch REST API&lt;/a&gt;.  I have implemented such things before, in different programming languages&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, and recently came on an excuse to write one in Rust; the need to have such thing has long since passed but the process of writing it has been a good opportunity to delve more into Rust, and think about how to implement such things.&lt;/p&gt;

&lt;h3 id=&quot;the-elasticsearch-api&quot;&gt;The ElasticSearch API&lt;/h3&gt;

&lt;p&gt;The ElasticSearch API on it’s surface is deceptively simple.  It appears there is a simple convention in its URLs: &lt;code&gt;/index/type/id&lt;/code&gt;, a RESTful convention for HTTP methods.  The documents it indexes and returns are JSON documents which can be easily embedded in the JSON payloads that are submitted when searching and returned from various operations.  And indeed, from many dynamic languages it is easy; that’s because those languages, on account of being dynamic and having simple literal syntax for maps, these arbitrary chunks of JSON can be embedded and manipulated without much headache.&lt;/p&gt;

&lt;p&gt;The first challenge, therefore, was to break away from that kind of thinking.  Since Rust is not a dynamic language, and since it’s design is very much aimed at systems programming, building an ElasticSearch client in Rust the same way I would in Clojure or Ruby would be both painful, and not taking advantage of Rust’s strengths.&lt;/p&gt;

&lt;p&gt;The cost of this has been the size of the library.  It is already significantly bigger than previous ElasticSearch clients, and I’ve only started implementing it; there are many large areas so far untouched (e.g. aggregations).  But the benefit is type-safety and hopefully self-explanatory code.  By using enums to specifically list all legitimate values of a parameter, for instance, many invalid combinations will be discovered at compile time.&lt;/p&gt;

&lt;h3 id=&quot;the-query-dsl&quot;&gt;The query DSL&lt;/h3&gt;

&lt;p&gt;Many of the API end-points are relatively simple: a struct with a few optional values.  The single biggest area of complication has been the &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html&quot;&gt;Query DSL&lt;/a&gt;; this consists of several dozen filters and queries, each of which has overlapping sets of slightly inconsistent options, many of which then go on to contain other filters and queries nested underneath.&lt;/p&gt;

&lt;p&gt;The first challenge was how to write all the structs and enums and builder-functions necessary to capture all of this.  That problem I solved by generating the majority of the code from templates.  I might write more about this in a future blog post, but the current implementation is a bit gnarly, I intend to refactor it now it’s mostly finished and I have the benefit of hindsight.&lt;/p&gt;

&lt;p&gt;The second challenge is theoretically smaller, but also slightly trickier.  This challenge is how to structure the Rust implementation so that users of my library can use this code without friction, and maybe even enjoy it.&lt;/p&gt;

&lt;h3 id=&quot;the-trivial-end-of-the-wedge&quot;&gt;The trivial end of the wedge&lt;/h3&gt;

&lt;p&gt;An example of the need to be friendly to potential users isn’t even ElasticSearch specific, it’s a common theme it seems with Rust code.  Under what circumstances should a function borrow a string using &lt;code&gt;&amp;amp;str&lt;/code&gt; and when should it require an owned &lt;code&gt;String&lt;/code&gt; be moved in to its grasp?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://doc.rust-lang.org/book/strings.html&quot;&gt;The Rust book recommends &lt;code&gt;&amp;amp;str&lt;/code&gt;&lt;/a&gt; on the grounds that it’s the cheapest option as it doesn’t force the user to allocate a &lt;code&gt;String&lt;/code&gt; especially.  This makes a lot of sense, especially seeing as a &lt;code&gt;String&lt;/code&gt; can be coerced to &lt;code&gt;&amp;amp;str&lt;/code&gt; just by simply doing &lt;code&gt;&amp;amp;owned_string&lt;/code&gt;.  But on the other hand, the Query DSL needs an owned &lt;code&gt;String&lt;/code&gt; because that’s what the Json library needs; and many users of my library will be dynamically creating strings especially anyway, so would almost certainly own them; moving those would make sense, otherwise the path would be &lt;code&gt;String&lt;/code&gt; to &lt;code&gt;&amp;amp;str&lt;/code&gt; to a brand new &lt;code&gt;String&lt;/code&gt; identical to the first one.  However, I didn’t want to enfore the use of &lt;code&gt;String&lt;/code&gt; everywhere either, because a significant proportion of use-cases would have various strings (such as index names) as effective constants; forcing the user to allocate these strings everytime is just anti-social, if the library needs it the library should do it.  I didn’t want &lt;code&gt;&quot;index_name&quot;.to_string()&lt;/code&gt; everywhere.&lt;/p&gt;

&lt;p&gt;The good thing about such fundamental questions is that there’s a good chance someone else has already thought about them, and indeed &lt;a href=&quot;http://hermanradtke.com/2015/05/06/creating-a-rust-function-that-accepts-string-or-str.html&quot;&gt;Rust has a solution built in&lt;/a&gt;, namely the &lt;a href=&quot;https://doc.rust-lang.org/std/convert/trait.Into.html&quot;&gt;&lt;code&gt;Into&lt;/code&gt; trait&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;By defining my functions to accept &lt;code&gt;Into&amp;lt;String&amp;gt;&lt;/code&gt; I could accept both &lt;code&gt;String&lt;/code&gt; and &lt;code&gt;&amp;amp;str&lt;/code&gt;.  If the user had an owned &lt;code&gt;String&lt;/code&gt;, happy days, it’ll be moved into place; if the user has a &lt;code&gt;&amp;amp;str&lt;/code&gt;, then the library will allocate a &lt;code&gt;String&lt;/code&gt; and carry on from there.  But the user-friendliness is preserved.  &lt;code&gt;my_function(&quot;constant_string&quot;)&lt;/code&gt; and &lt;code&gt;my_function(format!(&quot;dynamic_{}&quot;, val))&lt;/code&gt; both work.&lt;/p&gt;

&lt;p&gt;It was at this stage I thought: “hang on a moment, if I accepted Into&lt;miscellaneoustype&gt; for everything, I could make any consumer code shorter and not lose any type-safety.&quot;&lt;/miscellaneoustype&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-more-complex-examples&quot;&gt;The more complex examples&lt;/h3&gt;

&lt;p&gt;ElasticSearch’s Query DSL has a number of dynamic context-specific fields.  Values can be a single value or an array; the content could be a string, an integer, or even another map containing GeoJSON.  This is a text-book example of Duck Typing, and clients in dynamic languages can just ignore it, just write the code you want and away you go.&lt;/p&gt;

&lt;p&gt;But my decision to go-with-the-flow regarding Rust, and therefore to make a type-safe client, meant I couldn’t do that.  But the first version was quite unpleasently verbose.  To show the evolution of my approach, let’s pick an example:&lt;/p&gt;

&lt;h4 id=&quot;geo-bounding-box-filter&quot;&gt;Geo Bounding Box filter&lt;/h4&gt;

&lt;p&gt;A &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/1.5/query-dsl-geo-bounding-box-filter.html&quot;&gt;Geo Bounding Box filter&lt;/a&gt; can be used to find documents which have a &lt;code&gt;geo_point&lt;/code&gt; within the defined box.  Sounds simple?  It gets complex due to the number of options that a developer can use to define the box: either the corners (top-left, and bottom-right) can be provided, or the four (top, left, bottom, right) values can be given independently, but if corners are used those points can be defined in terms of lat-lng pairs or can be geohashes.  I decided to support all of these options rather than force any consumers of &lt;code&gt;rs-es&lt;/code&gt; to using a subset.&lt;/p&gt;

&lt;p&gt;There are certain options that can be ignored, however.  ElasticSearch allows lat-lng pairs to be defined in a number of ways, either JSON:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;location&amp;quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;quot;lon&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;-10.5&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;or arrays: &lt;code&gt;[-10.5, 50.5]&lt;/code&gt; (note the lng-lat ordering), or even strings: &lt;code&gt;&quot;50.5, -10.5&quot;&lt;/code&gt; (note the lat-lng ordering).  All three are equivalent, so I can generate one and ignore the others.&lt;/p&gt;

&lt;p&gt;So to begin with, I need a enum defining choices for &lt;code&gt;GeoBox&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span class=&quot;k&quot;&gt;pub&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and another one for the choice of &lt;code&gt;Location&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span class=&quot;k&quot;&gt;pub&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;LatLon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;GeoHash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Putting it all together is where the horrible verbosity becomes apparent:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span class=&quot;n&quot;&gt;Filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_geo_bounding_box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;pin&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;with_geo_box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LatLon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                  &lt;span class=&quot;n&quot;&gt;Location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LatLon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Not great compared to the JSON it produces, although arguably easier to understand:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;geo_bounding_box&amp;quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;quot;pin&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;nt&quot;&gt;&amp;quot;top_left&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;&amp;quot;lon&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;-10.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
       &lt;span class=&quot;nt&quot;&gt;&amp;quot;bottom_right&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;&amp;quot;lon&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;-10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;so-traits&quot;&gt;So… traits?&lt;/h3&gt;

&lt;p&gt;The solution to the verbosity problem was obvious after solving my &lt;code&gt;String&lt;/code&gt; vs. &lt;code&gt;&amp;amp;str&lt;/code&gt; problem.  I would define the &lt;code&gt;with_geo_box&lt;/code&gt; function, and any other such function, to take anything that implements &lt;code&gt;Into&amp;lt;GeoBox&amp;gt;&lt;/code&gt; rather than just &lt;code&gt;GeoBox&lt;/code&gt;.  This means that the full verbose option still works if you want to write it in full, but this approach also allows various shortcuts.&lt;/p&gt;

&lt;p&gt;For example, the verbose example above could be written:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span class=&quot;n&quot;&gt;Filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_geo_bounding_box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;pin&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;with_geo_box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because the tuple &lt;code&gt;((f64, f64), (f64, f64))&lt;/code&gt; implements &lt;code&gt;Into&amp;lt;GeoBox&amp;gt;&lt;/code&gt;.  Similar provisions are made for &lt;code&gt;(f64, f64, f64, f64)&lt;/code&gt; for the &lt;code&gt;Vertices&lt;/code&gt; version, and for &lt;code&gt;(String, String)&lt;/code&gt; for the geohash version.  This is achieved by simply implementing the &lt;code&gt;From&amp;lt;whatever&amp;gt; for Geobox&lt;/code&gt; trait for each required combination:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span class=&quot;k&quot;&gt;impl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;From&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;p&gt;Of course having to write five nearly identical lines for very similar functions has a high noise-to-signal ratio, but fortunately Rust has macros, after defining a couple of macros the above then becomes a one-liner:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span class=&quot;n&quot;&gt;from_exp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;f64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GeoBox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The code behind this is in the template that the code-generator uses to produce the full implementation of the Query DSL.  This can be &lt;a href=&quot;https://github.com/benashford/rs-es/blob/master/templates/query.rs.erb#L27&quot;&gt;seen here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is one final, but quite significant, advantage to the use of these conversion traits; the fact that any application could implement their own.  It would be quite likely that a hypothetical future application that needs a filter such as &lt;code&gt;geo_bounding_box&lt;/code&gt; would already have defined something analogous to a &lt;code&gt;GeoBox&lt;/code&gt;; rather than having to convert between the two at the point where a search happens, the &lt;code&gt;From&amp;lt;OtherType&amp;gt;&lt;/code&gt; trait could be implemented for &lt;code&gt;GeoBox&lt;/code&gt; allowing it to be dropped straight in.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This has been applied essentially everywhere in the code that implements the Query DSL, and it shows that using simple traits can have a big impact in the design of APIs.&lt;/p&gt;

&lt;p&gt;There are still many challenges, as various parts of the ElasticSearch API are subtly inconsistent however.&lt;/p&gt;

&lt;h5 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h5&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;I’ve written 2.5 such clients in Clojure alone, the .5 is a half-finished implementation I’m intending on open-sourcing eventually. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;And its related cousin, the &lt;a href=&quot;https://doc.rust-lang.org/std/convert/trait.From.html&quot;&gt;&lt;code&gt;From&lt;/code&gt; trait&lt;/a&gt;. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 24 May 2015 00:00:00 +0100</pubDate>
        <link>http://benashford.github.io/blog/2015/05/24/rust-traits-for-developer-friendly-libraries</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2015/05/24/rust-traits-for-developer-friendly-libraries</guid>
        
        <category>rust</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>group-by and Transducers</title>
        <description>&lt;p&gt;One of the random, but very useful, functions in &lt;code&gt;clojure.core&lt;/code&gt; is the venerable &lt;a href=&quot;https://clojure.github.io/clojure/clojure.core-api.html#clojure.core/group-by&quot;&gt;&lt;code&gt;group-by&lt;/code&gt;&lt;/a&gt;.  Simply take a collection of something, a function to extract a key, and it returns a map of the same data indexed by the key.  It’s incredibly useful, I use it all the time.&lt;/p&gt;

&lt;p&gt;But it’s also annoying.  Why?  Because often, perhaps even the majority of the time, I want to do some something with the values after they’ve been grouped; something that couldn’t be done before hand.  Examples of this include: a) removing the key used to index the data, b) applying functions that only make sense post-grouping - e.g. removing duplicate values.&lt;/p&gt;

&lt;p&gt;Let use that first example to go into more detail.  I often end-up dealing with data that is essentially a series of pairs, where the first element is an ID of some description, and the second is the data, often a map of some kind.  Using completely made-up data, it often looks like this:&lt;/p&gt;

&lt;pre&gt;
[[:a 1] [:a 2] [:b 3] [:a 4] [:c 5] [:b 6]]
&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;group-by&lt;/code&gt; on this gives me:&lt;/p&gt;

&lt;pre&gt;
{:a [[:a 1] [:a 2] [:a 4]], :b [[:b 3] [:b 6]], :c [[:c 5]]}
&lt;/pre&gt;

&lt;p&gt;Which groups by the first element, as I ask, but the values contain much redundant data.  So, of course I remove them afterwards, but this leads to ugly code as I iterate over this first map and produce a second one:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/7b9820e56f510c19b1c3.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The uglyness can be easily addressed by making this a generalised function so I’d never need to see the internals.  Just as &lt;code&gt;group-by&lt;/code&gt; takes a function to extract the key, this better group-by could take two functions - one to extract the key, the second to extract the value.  Although this does still leave a problem of intermediate short-lived maps and sequences.  I might as well not call the standard &lt;code&gt;group-by&lt;/code&gt; at all, and just implement it all myself:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/198bd6bc0ccb4220bda8.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This maintains the key contract of &lt;code&gt;group-by&lt;/code&gt;, namely returning a map with vectors as values, but strips out the data I’m not interested in:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (group-by-better first second example-data)
{:c [5], :b [3 6], :a [1 2 4]}
&lt;/pre&gt;

&lt;p&gt;Much better.  But hang on a minute, what about my other example.  If I wanted to do other group-specific processing, like removing duplicates, I need to take apart the map and put it back together again; either that or create a number of distinct group-by functions for different circumstances.  Neither of which felt like a particularly happy solution.&lt;/p&gt;

&lt;h3 id=&quot;transducers&quot;&gt;Transducers&lt;/h3&gt;

&lt;p&gt;Then, of course, the repressed memories of the various blog posts and videos regarding &lt;a href=&quot;http://clojure.org/transducers&quot;&gt;transducers&lt;/a&gt; all came back.  Is this, after all, the same sort of thing that transducers were invented for?&lt;/p&gt;

&lt;p&gt;I won’t try and explain in detail what transducers are, or how they work; many people with better credentials have tried, and still left confusion in their wake.  See the HN discussion on the original announcement: &lt;a href=&quot;https://news.ycombinator.com/item?id=8143905&quot;&gt;https://news.ycombinator.com/item?id=8143905&lt;/a&gt;.  But they’re not as complex as they might sound, if I were to try and explain in one single gross over-simplification I’d say they are an application of a higher-order function, but with “applied to” bit removed.  So, in a usual application of a higher-order function you’d apply it to some data: &lt;code&gt;(map function data)&lt;/code&gt;; but in Clojure 1.7: &lt;code&gt;(map function)&lt;/code&gt; returns a transducer that can then be applied to, well, anything.  A transducer could be applied over a collection, which would behave the same way as the pre-existing higher-order functions, but it doesn’t have to be; it could, in theory, be applied during a group-by operation to achieve the goals I mentioned above in a single operation without temporary maps and sequences…&lt;/p&gt;

&lt;p&gt;So, if I were to implement a &lt;code&gt;group-by-with-transducer&lt;/code&gt; function (I can think of a better name later!) what parameters would it take?  It would still need a function to extract a key for the data to be grouped by, then a transducer for what happens next, finally the data to be grouped.  The equivalent of my previous &lt;code&gt;group-by-better&lt;/code&gt; example above would be: &lt;code&gt;(group-by-with-transducer first (map second) data)&lt;/code&gt;.  This looks good to me, it reads as it does: group by the first element of the data, then map second over the results.&lt;/p&gt;

&lt;p&gt;How would one implement such a function?  Well, for starters, transducers can be stateful; this has two specific implications for this function: 1) we need one instance of the transducer for each group, and 2) we need to explicitly close the transducer at the end, to ensure that any state is flushed.  Secondly, transducers can terminate early (think of the &lt;code&gt;take&lt;/code&gt; function as an example), this needs to be taken into account; we can stop sending data through the transducer instance when this happens (although we do still need to close it).&lt;/p&gt;

&lt;p&gt;What would this function look like?  A bit like this:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/a9ea85225f4984174235.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The parameter &lt;code&gt;xf&lt;/code&gt; is the transducer.  &lt;code&gt;ff&lt;/code&gt; is a function that returns the particular instance of the transducer for the given key; here we use &lt;code&gt;conj&lt;/code&gt; as the reducing function for this instance of the transducer because we’re building a vector for the value in the map.&lt;/p&gt;

&lt;p&gt;The middle-part of the function is essentially the same as the &lt;code&gt;group-by-better&lt;/code&gt; function before, but with the difference of passing each element of &lt;code&gt;data&lt;/code&gt; through the instance of the transducer.  This “instance of the transducer”, as I’ve been calling it, is now performing the same action as &lt;code&gt;conj&lt;/code&gt; did in the non-transducer examples; transducers turn one reducing function into another one.  So if we called this function with &lt;code&gt;(map second)&lt;/code&gt; as the value of &lt;code&gt;xf&lt;/code&gt;, the function is doing the same thing as &lt;code&gt;(conj v (second d))&lt;/code&gt;.  And now suddenly transducers all make sense.&lt;/p&gt;

&lt;p&gt;The final line of the function is closing the transducer.  For stateless transducers this is a no-op, but for stateful transducers like the result of &lt;code&gt;(partition-all 3)&lt;/code&gt; it flushes anything remaining, and for any reduced values (e.g. &lt;code&gt;take&lt;/code&gt;) this unwraps it and returns the actual value.  The final line also makes use of a transducer, which is interesting.&lt;/p&gt;

&lt;h3 id=&quot;testing&quot;&gt;Testing&lt;/h3&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (group-by-with-transducer first (map second) example-data)
{:c [5], :b [3 6], :a [1 2 4]}
&lt;/pre&gt;

&lt;p&gt;Works a treat.  Let’s try it with a stateful transducer to make sure that works, I’ll partition the values of the map using &lt;code&gt;(paritition-all 2)&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (def xf (comp (map second) (partition-all 2)))
#&#39;group-transduce.core/xf
group-transduce.core&amp;gt; (group-by-with-transducer first xf example-data)
{:c [[5]], :b [[3 6]], :a [[1 2] [4]]}
&lt;/pre&gt;

&lt;p&gt;Let’s also try with an early-terminating operation:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (group-by-with-transducer first (comp (map second) (take 2)) example-data)
{:c [5], :b [3 6], :a [1 2]}
&lt;/pre&gt;

&lt;p&gt;Nice.&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;There are two advantages of this transducer based approach.&lt;/p&gt;

&lt;p&gt;Firstly, it’s less code.  We have one reusable function &lt;code&gt;group-by-with-transducer&lt;/code&gt; (note to self: really must find a better name), and any subsequent operations are composable predictable transducers.&lt;/p&gt;

&lt;p&gt;Secondly, it ought to be faster.  There’s less intermediate data, it’s just function calls.  But, notwithstanding my previous warnings &lt;a href=&quot;/blog/2014/12/07/the-folly-of-benchmarks/&quot;&gt;on the folly of benchmarking&lt;/a&gt;, I can’t make claims on performance without testing it.&lt;/p&gt;

&lt;p&gt;But first we need to invent a meaningful test.  Let’s use my original use-case.  We have a vector which is a series of 1,000 pairs; we want to group by the first value of each pair, then collect the second removing duplicates as we go.  Then I can compare an old-school implemetation with one that calls &lt;code&gt;group-by-with-transducer&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;the-test-data&quot;&gt;The test data&lt;/h4&gt;

&lt;p&gt;I’ll create some randomly generated test data to use:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (def ks [:a :b :c :d :e :f :g :h :i :j])
#&#39;group-transduce.core/ks
group-transduce.core&amp;gt; (def vs (into [] (range 50)))
#&#39;group-transduce.core/vs
group-transduce.core&amp;gt; (def test-data (take 1000 (repeatedly (fn [] [(rand-nth ks) (rand-nth vs)]))))
#&#39;group-transduce.core/test-data
group-transduce.core&amp;gt; (take 5 test-data)
([:c 13] [:j 38] [:j 3] [:f 19] [:g 12])
&lt;/pre&gt;

&lt;h4 id=&quot;the-old-school-version-without-using-transducers&quot;&gt;The old-school version (without using transducers)&lt;/h4&gt;

&lt;p&gt;Creating a function to use as the ‘control’ in an experiment is always controversial.  It would be very easy to create a bad implementation, just to show how dramatically better the new version is; similarly it would be easy to create a custom function that does exactly what it needs to do, this would be fast, but wouldn’t be the type of code you would see in-the-wild.  For the purposes of this test I will use a variation of the example I gave in the opening paragraphs:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/5ac62f26a6d568f2f244.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This does test both composing standard functions where possible with performing post-grouping actions in a fairly direct way.&lt;/p&gt;

&lt;p&gt;How does this perform?  Tested with Clojure 1.7.0alpha4, using the Criterium library to benchmark the function:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (bench (complex-example-1 test-data))
Evaluation count : 70620 in 60 samples of 1177 calls.
             Execution time mean : 857.557453 µs
    Execution time std-deviation : 19.955623 µs
   Execution time lower quantile : 821.684737 µs ( 2.5%)
   Execution time upper quantile : 895.128919 µs (97.5%)
                   Overhead used : 1.764232 ns
&lt;/pre&gt;

&lt;p&gt;The headline number is 0.858ms to transform the data as described.&lt;/p&gt;

&lt;h4 id=&quot;the-new-version-using-transducers&quot;&gt;The new version (using transducers)&lt;/h4&gt;

&lt;p&gt;The first hurdle here is that I need a &lt;code&gt;distinct&lt;/code&gt; transducer, but there isn’t one in the standard library.  So I will create one myself&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, I base this on how other transducers are implemented; you will also see it is a stateful transducer.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/94307b42547029e9fba1.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Let’s just test this to make sure it works:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (group-by-with-transducer first (comp (map second) (distinct-transducer)) [[:a 1] [:b 2] [:b 1] [:b 2]])
{:b [2 1], :a [1]}
&lt;/pre&gt;

&lt;p&gt;And then plug it in to our test harness:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/6461848ad579d271f163.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;And then benchmark it using the same test data as the previous benchmark:&lt;/p&gt;

&lt;pre&gt;
group-transduce.core&amp;gt; (bench (complex-example-2 test-data))
                Evaluation count : 85080 in 60 samples of 1418 calls.
             Execution time mean : 713.997396 µs
    Execution time std-deviation : 11.890208 µs
   Execution time lower quantile : 695.413537 µs ( 2.5%)
   Execution time upper quantile : 731.740410 µs (97.5%)
                   Overhead used : 1.764232 ns
&lt;/pre&gt;

&lt;p&gt;The headline number is 0.714ms.  Which is faster by 0.144ms, or 17% faster… not to be sniffed at.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The time saved by not using intermediate sequences and data-structures is actually quite significant.  Add to this the fact that transducers allow kinds of composition that weren’t possible before, not without constructing multiple versions of the similar functions, and it immediately seems that transducers are the answer to a wide array of situations.&lt;/p&gt;

&lt;h5 id=&quot;footnotes&quot;&gt; Footnotes&lt;/h5&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;See my previous controversy disclaimer. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 27 Dec 2014 00:00:00 +0000</pubDate>
        <link>http://benashford.github.io/blog/2014/12/27/group-by-and-transducers</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2014/12/27/group-by-and-transducers</guid>
        
        <category>clojure</category>
        
        <category>transducers</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Rust</title>
        <description>&lt;p&gt;Like every other developer on the internet, I’ve been watching the rise of &lt;a href=&quot;http://www.rust-lang.org&quot;&gt;Rust&lt;/a&gt; with interest.  I’ve spent the majority of my career in high-level language land, although I did do a lot of C++ once upon a time, and Rust may have passed me by with it’s focus around low-level concerns, bringing back bad memories of C++&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.  However, the sheer buzz surrounding it, and the fact that they’re now close to version 1.0 made me take another look.&lt;/p&gt;

&lt;p&gt;And I’m glad I did take another look as it has rapidly progressed since the last time I looked into it.  It’s good to see a new language with new ideas, rather than a “a mix of X and Y” which seems more common.  It bills itself as a “systems language”, which it certainly is, but it also has a wide-array of high-level features too - first-class functions, higher-order functions, etc., being particularly interesting to see as someone with prior experience of functional programming languages.&lt;/p&gt;

&lt;p&gt;Inspired by this, I decided to write something in Rust.  Particularly the &lt;a href=&quot;/blog/2014/03/22/the-power-of-lazy-sequences&quot;&gt;LazySort algorithm&lt;/a&gt; I’ve used for testing previously&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;equivalences-and-differences&quot;&gt;Equivalences and differences&lt;/h3&gt;

&lt;p&gt;The first and most obvious difference between Clojure and Rust is that Rust is statically typed, and very strictly statically typed too, there’s no Java-style bodging and casting, things aren’t nullable by default.  This means there will be no general &lt;code&gt;lazy-seq&lt;/code&gt; as per Clojure.&lt;/p&gt;

&lt;p&gt;The second and most fundamental difference between Rust and, well, everything really, is it’s approach to memory management.  It is not garbage collected, nor does it have C-style manual memory management&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.  Instead it has a concept of a lifetime: everything allocated has a deterministic point where it will be deallocated; and passing these values around by reference involves the compiler checking that the borrowed reference will not out-live the lifetime.  To return a value up the stack, higher than the place it was allocated, you either need to: copy the value, or return a boxed value, which is a special type of reference that can be moved to a different owner.  This is only the tip-of-the-iceberg into the compile-time checking Rust performs, it also protects against data-races by allowing only one mutable reference to exist at any point-in-time, for example.&lt;/p&gt;

&lt;p&gt;But it does have something that behaves a bit like a Clojure lazy sequence: the &lt;a href=&quot;http://doc.rust-lang.org/std/iter/trait.Iterator.html&quot;&gt;&lt;code&gt;Iterator&lt;/code&gt;&lt;/a&gt;.  So in theory it will be possible to create a kind of ‘sorted’ iterator that will return the values in a collection in sorted order, by sorting the underlying collection element-by-element, in the same way the Clojure implementation did.&lt;/p&gt;

&lt;h3 id=&quot;the-implementation&quot;&gt;The implementation&lt;/h3&gt;

&lt;p&gt;The finished version is available here: &lt;a href=&quot;https://github.com/benashford/rust-lazysort&quot;&gt;https://github.com/benashford/rust-lazysort&lt;/a&gt;, and also on &lt;a href=&quot;http://crates.io&quot;&gt;crates.io&lt;/a&gt; (the Rust package platform) here: &lt;a href=&quot;https://crates.io/crates/lazysort&quot;&gt;https://crates.io/crates/lazysort&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One of the higher-level features that Rust supports is extension methods, which proved very useful in this case, it enabled me to add a method to any iterator which would convert it into a sorted iterator.  This allows it to be chained with other Iterator functions:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/ce690c260fe953f66b3f.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The compiler’s impressive type-checking is so sophisticated that trying to call &lt;code&gt;sorted()&lt;/code&gt; on an iterator of &lt;code&gt;T&lt;/code&gt; is a compile-error unless &lt;code&gt;T&lt;/code&gt; implements the &lt;a href=&quot;http://doc.rust-lang.org/std/cmp/trait.Ord.html&quot;&gt;&lt;code&gt;Ord&lt;/code&gt;&lt;/a&gt; trait, which means the type has a default order.  The function &lt;code&gt;sorted&lt;/code&gt; simply doesn’t exist otherwise.  There is a second function &lt;code&gt;sorted_by&lt;/code&gt; that can be called on any iterator, regardless of content, this requires that a closure be given defining the order:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/74bf5caf61551ae27449.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;The extension methods are hygenic in the sense that they only apply if you import them:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/cf938a57b440d9f92f37.js&quot;&gt;&lt;/script&gt;

&lt;h4 id=&quot;the-algorithm&quot;&gt;The algorithm&lt;/h4&gt;

&lt;p&gt;Originally it was a fairly naive port of the Clojure version, i.e. it used immutable vectors containing the broken-down underlying data as it was gradually sorted.  This was later replaced with a version that was more idiomatic for Rust.  It was the same algorithm, a lazy quicksort, but the second one was in-place rather than immutable.  Although it worked on a copy of the underlying data, or original vector (or whatever it may have been) was untouched and didn’t need to be marked as mutable.&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Rust aims for C++-levels of performance, which is a very high-bar indeed.  It uses LLVM for the final compilation steps, so it already takes advantage of a mature compiler.  Rust is also a fast-moving target, so much so that during the short period I was working on this, testing it against nightly builds the measured performance got noticibly faster without me having to do anything.&lt;/p&gt;

&lt;p&gt;This, however, made judging the utility of lazy-sorting as a concept really quite difficult.  At one stage the cost of lazily-sorting a full large vector was only 15% worse than the sort function in the standard library; small enough that a developer could consider using it for all purposes, not just the usual optimisation for short-cutting the sorting process of only partially consuming the full set of data.  But at the time of writing, even though my code got faster with the latest nightly builds of Rust, the standard library sort got much, much faster!  The end result is that the overhead for a full large vector is around 2.5 times, so probably not something you’d want to use all the time.&lt;/p&gt;

&lt;p&gt;However the same benefits as found in the Clojure version were also found in the Rust version - e.g. taking the top 1,000 results.  In the tests of 50,000 random numbers, the cross-over point is just over 10,000.  Interestingly this is much better than the Clojure version, where the cross-over is around 1,250.&lt;/p&gt;

&lt;p&gt;The headline numbers are that the Rust version is significantly faster than the Clojure version.  Using the test of picking the top 1,000 out of 50,000 random numbers; the Clojure version manages an average of 11.2ms, the Rust version &lt;a href=&quot;https://github.com/benashford/rust-lazysort#implementation-details-and-performance&quot;&gt;0.9ms&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h3&gt;

&lt;p&gt;Bring on version 1.0.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;I appreciate that C++ has changed a lot since then.  In those dark days finding a compiler that supported STL was a nice surprise. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Don’t worry, the next blog post will be on a different subject entirely. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Not strictly true, depending on how deep you want to get into &lt;code&gt;unsafe&lt;/code&gt; blocks. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 21 Dec 2014 00:00:00 +0000</pubDate>
        <link>http://benashford.github.io/blog/2014/12/21/rust</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2014/12/21/rust</guid>
        
        <category>clojure</category>
        
        <category>rust</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>The folly of benchmarks</title>
        <description>&lt;p&gt;In my previous post &lt;a href=&quot;/blog/2014/03/22/the-power-of-lazy-sequences&quot;&gt;The power of lazy sequences&lt;/a&gt; I fell in to a trap.  A common trap, one which most of us fall, some of us quite regularly.  I talk, of course, of trying to measure execution time.&lt;/p&gt;

&lt;p&gt;In my defence, given the context of that post, some kind of elapsed-time measurement was needed as it would have been mostly conjecture otherwise.  Accurately measuring elapsed time, however, is actually quite difficult.  There were two specific problems with my approach: first, I was using &lt;code&gt;clojure.core&lt;/code&gt;’s &lt;code&gt;time&lt;/code&gt; function&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;; second, I had not paid any attention to any JVM parameters.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;time&lt;/code&gt; simply measures a single invocation, and while I had manually called each function a number of times before taking my measurements, and the elapsed time had appeared to stabilise&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, there was still a degree of varience on each call; it wasn’t exactly scientific.&lt;/p&gt;

&lt;p&gt;Ignoring the JVM settings was an even bigger error, as it turns out that Leiningen disables certain JVM optimisations in order to achieve faster load times.  A deployed uberjar will be running with default settings, but running the code via &lt;code&gt;lein repl&lt;/code&gt; will be slower.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;  You may think that this doesn’t matter, as it’s the same for both tests, but there is an unknown factor of how the disabled JVM transformations would have impacted each one; in theory, the lazy version has the most to gain as it has the most method calls, which could be inlined, and also temporary objects, which could be stack allocated.  That final claim is conjecture, so let’s put it to the test!&lt;/p&gt;

&lt;h4 id=&quot;the-test-and-results&quot;&gt;The test and results&lt;/h4&gt;

&lt;p&gt;First, I stop Leiningen from changing any JVM settings, the test will now run with the default JVM settings.  Second, I use a proper benchmarking tool designed to deal with the difficulties of measuring execution time on the JVM, specifically I’m using Hugo Duncan’s &lt;a href=&quot;https://github.com/hugoduncan/criterium&quot;&gt;Criterium&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The test was the same as the previous post.  50,000 random integers between zero and 1,000,000 were generated; the first 100 ordered naturally were selected.  The test was performed on the same hardware.&lt;/p&gt;

&lt;p&gt;The results for the eager sort of the entire vector, followed by taking the first 100:&lt;/p&gt;

&lt;pre&gt;
lazysort.core&amp;gt; (bench (do (doall (take 100 (sort rand-nums))) nil))
WARNING: Final GC required 2.890797395019582 % of runtime
Evaluation count : 2760 in 60 samples of 46 calls.
Execution time mean : 22.063202 ms
Execution time std-deviation : 489.021271 µs
Execution time lower quantile : 21.444953 ms ( 2.5%)
Execution time upper quantile : 23.258641 ms (97.5%)
Overhead used : 1.777138 ns

Found 3 outliers in 60 samples (5.0000 %)
low-severe   3 (5.0000 %)
Variance from outliers : 10.9452 % Variance is moderately inflated by outliers
&lt;/pre&gt;

&lt;p&gt;With the default JVM options enabled, the elapsed time went from 39.6ms to 22.1ms (reduced by 44.2%).&lt;/p&gt;

&lt;p&gt;The results for the lazy sort, taking the first 100:&lt;/p&gt;

&lt;pre&gt;
lazysort.core&amp;gt; (bench (do (doall (take 100 (lazysort rand-nums))) nil))
Evaluation count : 11640 in 60 samples of 194 calls.
Execution time mean : 5.086969 ms
Execution time std-deviation : 61.738542 µs
Execution time lower quantile : 4.966331 ms ( 2.5%)
Execution time upper quantile : 5.220216 ms (97.5%)
Overhead used : 1.777138 ns

Found 2 outliers in 60 samples (3.3333 %)
low-severe   2 (3.3333 %)
Variance from outliers : 1.6389 % Variance is slightly inflated by outliers
&lt;/pre&gt;

&lt;p&gt;In this case the elapsed time went from 10.9ms to 5.1ms (reduced by 53.2%).&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;The execution time of both was significantly improved, and indeed the lazy version gained the most.  The real conclusion however: there are good tools for this sort of thing which are better than drawing conclusions from ad-hoc timing code.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes &lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Not that there is anything wrong how that function is implemented, just that it is better suited for timing real-world situations rather than benchmarks. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;The first call is almost always the slowest, as the JVM will be loading any referenced classes etc.  Subsequent calls may still be slow, as the Hotspot compiler does its work. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/technomancy/leiningen/wiki/Faster#tiered-compilation&quot;&gt;https://github.com/technomancy/leiningen/wiki/Faster#tiered-compilation&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 07 Dec 2014 00:00:00 +0000</pubDate>
        <link>http://benashford.github.io/blog/2014/12/07/the-folly-of-benchmarks</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2014/12/07/the-folly-of-benchmarks</guid>
        
        <category>clojure</category>
        
        <category>benchmarking</category>
        
        <category>jvm</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>The power of lazy sequences</title>
        <description>&lt;p&gt;The selling-pitch for the concept of lazy evaluation includes the claim that they can, if used in the right way, improve performance.  Indeed, the new Stream API in Java 8 vaguely references that &lt;a href=&quot;http://download.java.net/jdk8/docs/api/java/util/stream/package-summary.html&quot;&gt;laziness exposes “opportunities for optimisation”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But how much effect can this have.  Well lets look at an example shall we: sorting in Clojure.&lt;/p&gt;

&lt;p&gt;Clojure, of course, has a &lt;code&gt;sort&lt;/code&gt; function as part of the core namespace&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.  This is defined as a thin wrapper around the sorting functionality of Java, i.e. it converts the collection into an array, then calls &lt;code&gt;sort&lt;/code&gt; on &lt;code&gt;java.util.Arrays&lt;/code&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, which ought to be pretty fast.  It takes the pragmatic approach of leveraging the underlying platform, same as so many other Clojure features.  This pragmatism means it will beat a purer implementation which embodies functional programming and immutable data structures.&lt;/p&gt;

&lt;p&gt;But what if we had a lazy sorting algorithm?&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/benashford/9716335.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This doesn’t make any optimisations beyond being lazy; it has no side-effects, it uses no mutable datastructures, so surely stands no chance against the built-in function?&lt;/p&gt;

&lt;p&gt;Let’s test it.  Let’s define a vector of 50000 random numbers, and sort it with both implementations:&lt;/p&gt;

&lt;pre&gt;
(def rand-nums (into [] (repeatedly 50000 #(rand-int 1000000))))
&lt;/pre&gt;

&lt;p&gt;Built-in &lt;code&gt;sort&lt;/code&gt; (note: the use of &lt;code&gt;doall&lt;/code&gt; is not strictly necessary, but is here because it’s needed when testing &lt;code&gt;lazysort&lt;/code&gt;, otherwise the sequence won’t be consumed and it will take no time at all):&lt;/p&gt;

&lt;pre&gt;
lazysort.core&amp;gt; (time (do (doall (sort rand-nums)) nil))
&quot;Elapsed time: 44.188 msecs&quot;
&lt;/pre&gt;

&lt;p&gt;And my &lt;code&gt;lazysort&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;
lazysort.core&amp;gt; (time (do (doall (lazysort rand-nums)) nil))
&quot;Elapsed time: 528.527 msecs&quot;
&lt;/pre&gt;

&lt;p&gt;Yep, as expected, it’s quite a bit slower.  So where, exactly, do the optimisations of lazy sequences manifest themselves?  One significant benefit is short-cutting.  Lazy evaluation can be stopped when no-further results are expected and/or needed, this could leave a significant amount of work undone.&lt;/p&gt;

&lt;p&gt;So going back to our vector of random numbers, what if we didn’t care about the precise order of everything?  What if we only wanted the first hundred items from the sorted sequence?&lt;/p&gt;

&lt;p&gt;Built-in &lt;code&gt;sort&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;
lazysort.core&amp;gt; (time (doall (take 100 (sort rand-nums))))
&quot;Elapsed time: 39.693 msecs&quot;
(11 60 80 88 110 131 132 145 178 179 193 216 253 256 311 344 354 381 424 424 477 478 520 527 646 658 676 677 684 696 716 721 737 775 812 821 843 848 864 902 939 939 947 949 949 962 969 980 989 1064 1069 1075 1173 1196 1199 1204 1209 1218 1236 1240 1285 1293 1346 1359 1369 1432 1477 1494 1508 1518 1553 1560 1603 1672 1710 1719 1772 1775 1795 1797 1824 1856 1864 1895 1932 1940 2020 2021 2075 2088 2098 2102 2105 2126 2143 2157 2164 2263 2263 2279)
&lt;/pre&gt;

&lt;p&gt;The performance is about the same, in the region of 40 milliseconds.  This is expected, of course, the whole 50,000 items are being sorted before the first 100 items are picked.  Let’s try &lt;code&gt;lazysort&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;
(time (doall (take 100 (lazysort rand-nums))))
&quot;Elapsed time: 10.894 msecs&quot;
(11 60 80 88 110 131 132 145 178 179 193 216 253 256 311 344 354 381 424 424 477 478 520 527 646 658 676 677 684 696 716 721 737 775 812 821 843 848 864 902 939 939 947 949 949 962 969 980 989 1064 1069 1075 1173 1196 1199 1204 1209 1218 1236 1240 1285 1293 1346 1359 1369 1432 1477 1494 1508 1518 1553 1560 1603 1672 1710 1719 1772 1775 1795 1797 1824 1856 1864 1895 1932 1940 2020 2021 2075 2088 2098 2102 2105 2126 2143 2157 2164 2263 2263 2279)
&lt;/pre&gt;

&lt;p&gt;It only requires a quarter of the time, being able to stop early saves significant time.  There are literally hundreds of cases when this kind of short-cut is done in large applications, and shows how much of a benefit lazy sequences and lazy evaluation generally can actually have.&lt;/p&gt;

&lt;h4 id=&quot;tradeoffs&quot;&gt;Tradeoffs&lt;/h4&gt;

&lt;p&gt;An earlier version of the &lt;code&gt;lazysort&lt;/code&gt; function used a &lt;code&gt;group-by&lt;/code&gt; rather than using &lt;code&gt;filter&lt;/code&gt; and &lt;code&gt;remove&lt;/code&gt; to obtain the items before and after the pivot.  My thinking was that although &lt;code&gt;group-by&lt;/code&gt; is an eager operation, not lazy, everything in the resulting sequence would be scanned once anyway and would save double-comparing each element.&lt;/p&gt;

&lt;p&gt;In practice however, this earlier version was twice as slow when reading the first 100 results for a sorted sequence, albeit faster when sorting the entire sequence.&lt;/p&gt;

&lt;p&gt;You can see the earlier version if you examine the history of the gist.&lt;/p&gt;

&lt;h4 id=&quot;update&quot;&gt;Update&lt;/h4&gt;

&lt;p&gt;The timings above are from the first published version of this post.  There are newer benchmarks, more accurately taken, in my newer post &lt;a href=&quot;/blog/2014/12/07/the-folly-of-benchmarks&quot;&gt;The folly of benchmarks&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://clojure.github.io/clojure/clojure.core-api.html#clojure.core/sort&quot;&gt;http://clojure.github.io/clojure/clojure.core-api.html#clojure.core/sort&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/clojure/clojure/blob/c6756a8bab137128c8119add29a25b0a88509900/src/clj/clojure/core.clj#L2742&quot;&gt;https://github.com/clojure/clojure/blob/c6756a8bab137128c8119add29a25b0a88509900/src/clj/clojure/core.clj#L2742&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;This implementation is mine, but it’s not an original idea.  It’s essentially Quicksort after-all. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 22 Mar 2014 00:00:00 +0000</pubDate>
        <link>http://benashford.github.io/blog/2014/03/22/the-power-of-lazy-sequences</link>
        <guid isPermaLink="true">http://benashford.github.io/blog/2014/03/22/the-power-of-lazy-sequences</guid>
        
        <category>clojure</category>
        
        <category>lazy-evaluation</category>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
